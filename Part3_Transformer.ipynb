{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18sd8-wHGDR0"
   },
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "$$\n",
    "# Part 3: Transformer\n",
    "<a id=part3></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AiCHKxjfGLRu"
   },
   "source": [
    "In this part we will implement a variation of the attention mechanism named the 'sliding window attention'. Next, we will create a transformer encoder with the sliding-window attention implementation, and we will train the encoder for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0t2UQaeKCKxa",
    "outputId": "10e4f165-d0c0-4ac9-960a-6796ff3c1750"
   },
   "outputs": [],
   "source": [
    "import unittest\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35B3L7USC5PE",
    "outputId": "5e05f218-468a-4e54-c9bb-a7357f0dfe09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "test = unittest.TestCase()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kRUk_GYNDd5"
   },
   "source": [
    "## Reminder: scaled dot product attention\n",
    "<a id=part3_1></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30P7CuIwNKy-"
   },
   "source": [
    "In class, you saw that the scaled dot product attention is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mat{B} &= \\frac{1}{\\sqrt{d}} \\mat{Q}\\mattr{K}  \\ \\in\\set{R}^{m\\times n} \\\\\n",
    "\\mat{A} &= softmax({\\mat{B}},{\\mathrm{dim}=1}), \\in\\set{R}^{m\\times n} \\\\\n",
    "\\mat{Y} &= \\mat{A}\\mat{V} \\ \\in\\set{R}^{m\\times d_v}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where `K`,`Q` and `V` for the self attention came as projections of the same input sequnce\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\vec{q}_{i} &= \\mat{W}_{xq}\\vec{x}_{i} &\n",
    "\\vec{k}_{i} &= \\mat{W}_{xk}\\vec{x}_{i} &\n",
    "\\vec{v}_{i} &= \\mat{W}_{xv}\\vec{x}_{i} \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "If you feel the attention mechanism doesn't quite sit right, we recommend you go over lecture and tutorial notes before proceeding. \n",
    "\n",
    "We are now going to introduce a slight variation of the scaled dot product attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8CMV4VKG7YB"
   },
   "source": [
    "## Sliding window attention\n",
    "<a id=part3_2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YphH4DuEHP1p"
   },
   "source": [
    "The scaled dot product attention computes the dot product between **every** pair of key and query vectors. Therefore, the computation complexity is $O(n^2)$ where $n$ is the sequence length.\n",
    "\n",
    "In order to obtain a computational complexity that grows linearly with the sequnce length, the authors of 'Longformer: The Long-Document Transformer https://arxiv.org/pdf/2004.05150.pdf' proposed the 'sliding window attention' which is a variation of the scaled dot product attention. \n",
    "\n",
    "In this variation, instead of computing the dot product for every pair of key and query vectors, the dot product is only computed for keys that are in a certain 'window' around the query vector. \n",
    "\n",
    "For example, if the keys and queries are embeddings of words in the sentence \"CS is more prestigious than EE\", and the window size is 2, then for the query corresponding to the word 'is' we will only compute a dot product with the keys that are at most ${window\\_size}\\over{2}$$ = $${2}\\over{2}$$=1$ to the left and to the right. Meaning the keys that correspond to the workds 'CS', 'is' and 'more'.\n",
    "\n",
    "Formally, the intermediate calculation of the normalized dot product can be written as: \n",
    "\n",
    "$$\n",
    "\\mathrm{b}(q, k, w) \n",
    "=\n",
    "\\begin{cases}\n",
    "    q⋅k^T\\over{\\sqrt{d_k}} & \\mathrm{if} \\;d(q,k) ≤ {{w}\\over{2}} \\\\\n",
    "    -\\infty & \\mathrm{otherwise}\n",
    "\\end{cases}.\n",
    "$$\n",
    "\n",
    "Where $b(\\cdot,\\cdot,\\cdot)$ is the intermediate result function (used to construct a matrix $\\mat{B}$ on which we perform the softmax), $q$ is the query vector, $k$ is the key vector, $w$ is the sliding window size, and $d(\\cdot,\\cdot)$ is the distance function between the positions of the tokens corresponding to the key and query vectors.\n",
    "\n",
    "**Note**: The distance function $d(\\cdot,\\cdot)$ is **Not** cyclical. Meaning that that in the example above when searching for the words at distance 1 from the word 'CS', we **don't** return cyclically from the right and count the word EE.\n",
    "\n",
    "The result of this operation can be visualized like this: (green corresponds to computing the scaled dot product, and white to a no-op or $-∞$).\n",
    "\n",
    "<img src=\"https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-31_at_7.27.29_PM.png\" width=\"400\"/>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZTy748cWIEC"
   },
   "source": [
    "**TODO**: Implement the sliding_window_attention function in hw3/transformer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "CY1gz0fSebQP",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial torch.Size([2, 5, 3])\n",
      "shapes  2 5 1 3\n",
      "torch.Size([2, 1, 5, 5])\n",
      "attn tensor([1., 1., 1., 1., 1.])\n",
      "gt tensor([[[5.5073e-03, 9.9449e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [8.8454e-19, 9.4050e-10, 1.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 2.5513e-32, 1.5973e-16, 1.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 1.4013e-45, 2.7127e-23, 1.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 4.6069e-30, 1.0000e+00]],\n",
      "\n",
      "        [[5.5073e-03, 9.9449e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [8.8454e-19, 9.4050e-10, 1.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 2.5513e-32, 1.5973e-16, 1.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 1.4013e-45, 2.7127e-23, 1.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 4.6069e-30, 1.0000e+00]]])\n",
      "initial torch.Size([2, 3, 5, 3])\n",
      "shapes  2 5 3 3\n",
      "torch.Size([2, 3, 5, 5])\n",
      "attn tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "from hw3.transformer import sliding_window_attention\n",
    "\n",
    "\n",
    "## test sliding-window attention\n",
    "num_heads = 3\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "embed_dim = 3\n",
    "window_size = 2\n",
    "\n",
    "## test without extra dimension for heads\n",
    "x = torch.arange(seq_len*embed_dim).reshape(seq_len,embed_dim).repeat(batch_size,1).reshape(batch_size, seq_len, -1).float()\n",
    "\n",
    "values, attention = sliding_window_attention(x, x, x,window_size)\n",
    "\n",
    "gt_values = torch.load(os.path.join('test_tensors','values_tensor_0_heads.pt'))\n",
    "\n",
    "# print(gt_values, values)\n",
    "test.assertTrue(torch.all(values == gt_values), f'the tensors differ in dims [B,row,col]:{torch.stack(torch.where(values != gt_values),dim=0)}')\n",
    "gt_attention = torch.load(os.path.join('test_tensors','attention_tensor_0_heads.pt'))\n",
    "print(\"gt\", gt_attention)\n",
    "\n",
    "test.assertTrue(torch.all(attention == gt_attention), f'the tensors differ in dims [B,row,col]:{torch.stack(torch.where(attention != gt_attention),dim=0)}')\n",
    "\n",
    "\n",
    "## test with extra dimension for heads\n",
    "x = torch.arange(seq_len*embed_dim).reshape(seq_len,embed_dim).repeat(batch_size, num_heads, 1).reshape(batch_size, num_heads, seq_len, -1).float()\n",
    "\n",
    "values, attention = sliding_window_attention(x, x, x,window_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "gt_values = torch.load(os.path.join('test_tensors','values_tensor_3_heads.pt'))\n",
    "test.assertTrue(torch.all(values == gt_values), f'the tensors differ in dims [B,num_heads,row,col]:{torch.stack(torch.where(values != gt_values),dim=0)}')\n",
    "\n",
    "\n",
    "gt_attention = torch.load(os.path.join('test_tensors','attention_tensor_3_heads.pt'))\n",
    "test.assertTrue(torch.all(attention == gt_attention), f'the tensors differ in dims [B,num_heads,row,col]:{torch.stack(torch.where(attention != gt_attention),dim=0)}')\n",
    "# print(\"gt\", gt_attention)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial torch.Size([2, 4, 10, 16])\n",
      "shapes  2 10 4 16\n",
      "torch.Size([2, 4, 10, 10])\n",
      "attn tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "real full  \n",
      " tensor([[[1.2400e+03, 3.1600e+03, 5.0800e+03, 7.0000e+03, 8.9200e+03,\n",
      "          1.0840e+04, 1.2760e+04, 1.4680e+04, 1.6600e+04, 1.8520e+04],\n",
      "         [3.1600e+03, 9.1760e+03, 1.5192e+04, 2.1208e+04, 2.7224e+04,\n",
      "          3.3240e+04, 3.9256e+04, 4.5272e+04, 5.1288e+04, 5.7304e+04],\n",
      "         [5.0800e+03, 1.5192e+04, 2.5304e+04, 3.5416e+04, 4.5528e+04,\n",
      "          5.5640e+04, 6.5752e+04, 7.5864e+04, 8.5976e+04, 9.6088e+04],\n",
      "         [7.0000e+03, 2.1208e+04, 3.5416e+04, 4.9624e+04, 6.3832e+04,\n",
      "          7.8040e+04, 9.2248e+04, 1.0646e+05, 1.2066e+05, 1.3487e+05],\n",
      "         [8.9200e+03, 2.7224e+04, 4.5528e+04, 6.3832e+04, 8.2136e+04,\n",
      "          1.0044e+05, 1.1874e+05, 1.3705e+05, 1.5535e+05, 1.7366e+05],\n",
      "         [1.0840e+04, 3.3240e+04, 5.5640e+04, 7.8040e+04, 1.0044e+05,\n",
      "          1.2284e+05, 1.4524e+05, 1.6764e+05, 1.9004e+05, 2.1244e+05],\n",
      "         [1.2760e+04, 3.9256e+04, 6.5752e+04, 9.2248e+04, 1.1874e+05,\n",
      "          1.4524e+05, 1.7174e+05, 1.9823e+05, 2.2473e+05, 2.5122e+05],\n",
      "         [1.4680e+04, 4.5272e+04, 7.5864e+04, 1.0646e+05, 1.3705e+05,\n",
      "          1.6764e+05, 1.9823e+05, 2.2882e+05, 2.5942e+05, 2.9001e+05],\n",
      "         [1.6600e+04, 5.1288e+04, 8.5976e+04, 1.2066e+05, 1.5535e+05,\n",
      "          1.9004e+05, 2.2473e+05, 2.5942e+05, 2.9410e+05, 3.2879e+05],\n",
      "         [1.8520e+04, 5.7304e+04, 9.6088e+04, 1.3487e+05, 1.7366e+05,\n",
      "          2.1244e+05, 2.5122e+05, 2.9001e+05, 3.2879e+05, 3.6758e+05]],\n",
      "\n",
      "        [[4.4924e+05, 4.9212e+05, 5.3500e+05, 5.7788e+05, 6.2076e+05,\n",
      "          6.6364e+05, 7.0652e+05, 7.4940e+05, 7.9228e+05, 8.3516e+05],\n",
      "         [4.9212e+05, 5.3910e+05, 5.8607e+05, 6.3305e+05, 6.8002e+05,\n",
      "          7.2700e+05, 7.7398e+05, 8.2095e+05, 8.6793e+05, 9.1490e+05],\n",
      "         [5.3500e+05, 5.8607e+05, 6.3714e+05, 6.8822e+05, 7.3929e+05,\n",
      "          7.9036e+05, 8.4143e+05, 8.9250e+05, 9.4358e+05, 9.9465e+05],\n",
      "         [5.7788e+05, 6.3305e+05, 6.8822e+05, 7.4338e+05, 7.9855e+05,\n",
      "          8.5372e+05, 9.0889e+05, 9.6406e+05, 1.0192e+06, 1.0744e+06],\n",
      "         [6.2076e+05, 6.8002e+05, 7.3929e+05, 7.9855e+05, 8.5782e+05,\n",
      "          9.1708e+05, 9.7634e+05, 1.0356e+06, 1.0949e+06, 1.1541e+06],\n",
      "         [6.6364e+05, 7.2700e+05, 7.9036e+05, 8.5372e+05, 9.1708e+05,\n",
      "          9.8044e+05, 1.0438e+06, 1.1072e+06, 1.1705e+06, 1.2339e+06],\n",
      "         [7.0652e+05, 7.7398e+05, 8.4143e+05, 9.0889e+05, 9.7634e+05,\n",
      "          1.0438e+06, 1.1113e+06, 1.1787e+06, 1.2462e+06, 1.3136e+06],\n",
      "         [7.4940e+05, 8.2095e+05, 8.9250e+05, 9.6406e+05, 1.0356e+06,\n",
      "          1.1072e+06, 1.1787e+06, 1.2503e+06, 1.3218e+06, 1.3934e+06],\n",
      "         [7.9228e+05, 8.6793e+05, 9.4358e+05, 1.0192e+06, 1.0949e+06,\n",
      "          1.1705e+06, 1.2462e+06, 1.3218e+06, 1.3975e+06, 1.4731e+06],\n",
      "         [8.3516e+05, 9.1490e+05, 9.9465e+05, 1.0744e+06, 1.1541e+06,\n",
      "          1.2339e+06, 1.3136e+06, 1.3934e+06, 1.4731e+06, 1.5529e+06]],\n",
      "\n",
      "        [[1.7164e+06, 1.8003e+06, 1.8841e+06, 1.9680e+06, 2.0518e+06,\n",
      "          2.1356e+06, 2.2195e+06, 2.3033e+06, 2.3872e+06, 2.4710e+06],\n",
      "         [1.8003e+06, 1.8882e+06, 1.9762e+06, 2.0641e+06, 2.1520e+06,\n",
      "          2.2400e+06, 2.3279e+06, 2.4158e+06, 2.5038e+06, 2.5917e+06],\n",
      "         [1.8841e+06, 1.9762e+06, 2.0682e+06, 2.1602e+06, 2.2522e+06,\n",
      "          2.3443e+06, 2.4363e+06, 2.5283e+06, 2.6204e+06, 2.7124e+06],\n",
      "         [1.9680e+06, 2.0641e+06, 2.1602e+06, 2.2563e+06, 2.3525e+06,\n",
      "          2.4486e+06, 2.5447e+06, 2.6409e+06, 2.7370e+06, 2.8331e+06],\n",
      "         [2.0518e+06, 2.1520e+06, 2.2522e+06, 2.3525e+06, 2.4527e+06,\n",
      "          2.5529e+06, 2.6531e+06, 2.7534e+06, 2.8536e+06, 2.9538e+06],\n",
      "         [2.1356e+06, 2.2400e+06, 2.3443e+06, 2.4486e+06, 2.5529e+06,\n",
      "          2.6572e+06, 2.7616e+06, 2.8659e+06, 2.9702e+06, 3.0745e+06],\n",
      "         [2.2195e+06, 2.3279e+06, 2.4363e+06, 2.5447e+06, 2.6531e+06,\n",
      "          2.7616e+06, 2.8700e+06, 2.9784e+06, 3.0868e+06, 3.1952e+06],\n",
      "         [2.3033e+06, 2.4158e+06, 2.5283e+06, 2.6409e+06, 2.7534e+06,\n",
      "          2.8659e+06, 2.9784e+06, 3.0909e+06, 3.2034e+06, 3.3159e+06],\n",
      "         [2.3872e+06, 2.5038e+06, 2.6204e+06, 2.7370e+06, 2.8536e+06,\n",
      "          2.9702e+06, 3.0868e+06, 3.2034e+06, 3.3200e+06, 3.4366e+06],\n",
      "         [2.4710e+06, 2.5917e+06, 2.7124e+06, 2.8331e+06, 2.9538e+06,\n",
      "          3.0745e+06, 3.1952e+06, 3.3159e+06, 3.4366e+06, 3.5573e+06]],\n",
      "\n",
      "        [[3.8028e+06, 3.9276e+06, 4.0524e+06, 4.1772e+06, 4.3020e+06,\n",
      "          4.4268e+06, 4.5516e+06, 4.6764e+06, 4.8012e+06, 4.9260e+06],\n",
      "         [3.9276e+06, 4.0565e+06, 4.1854e+06, 4.3143e+06, 4.4432e+06,\n",
      "          4.5721e+06, 4.7010e+06, 4.8299e+06, 4.9588e+06, 5.0877e+06],\n",
      "         [4.0524e+06, 4.1854e+06, 4.3184e+06, 4.4514e+06, 4.5844e+06,\n",
      "          4.7174e+06, 4.8504e+06, 4.9834e+06, 5.1164e+06, 5.2494e+06],\n",
      "         [4.1772e+06, 4.3143e+06, 4.4514e+06, 4.5885e+06, 4.7256e+06,\n",
      "          4.8627e+06, 4.9998e+06, 5.1369e+06, 5.2739e+06, 5.4110e+06],\n",
      "         [4.3020e+06, 4.4432e+06, 4.5844e+06, 4.7256e+06, 4.8668e+06,\n",
      "          5.0080e+06, 5.1491e+06, 5.2903e+06, 5.4315e+06, 5.5727e+06],\n",
      "         [4.4268e+06, 4.5721e+06, 4.7174e+06, 4.8627e+06, 5.0080e+06,\n",
      "          5.1532e+06, 5.2985e+06, 5.4438e+06, 5.5891e+06, 5.7344e+06],\n",
      "         [4.5516e+06, 4.7010e+06, 4.8504e+06, 4.9998e+06, 5.1491e+06,\n",
      "          5.2985e+06, 5.4479e+06, 5.5973e+06, 5.7466e+06, 5.8960e+06],\n",
      "         [4.6764e+06, 4.8299e+06, 4.9834e+06, 5.1369e+06, 5.2903e+06,\n",
      "          5.4438e+06, 5.5973e+06, 5.7507e+06, 5.9042e+06, 6.0577e+06],\n",
      "         [4.8012e+06, 4.9588e+06, 5.1164e+06, 5.2739e+06, 5.4315e+06,\n",
      "          5.5891e+06, 5.7466e+06, 5.9042e+06, 6.0618e+06, 6.2194e+06],\n",
      "         [4.9260e+06, 5.0877e+06, 5.2494e+06, 5.4110e+06, 5.5727e+06,\n",
      "          5.7344e+06, 5.8960e+06, 6.0577e+06, 6.2194e+06, 6.3810e+06]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1280).reshape((2,4,10,16)).float()\n",
    "full = torch.matmul(x, x.transpose(-1,-2))\n",
    "values, attention = sliding_window_attention(x, x, x,4)\n",
    "print(\"real full \" ,\"\\n\", full[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 3, 4])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.randn(2,5,3)\n",
    "t = F.pad(t, (0,0,2,2,0,0))\n",
    "t.shape\n",
    "t = t.unfold(-2, 4, 1)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TlHiDHhgeS1_"
   },
   "source": [
    "## Multihead Sliding window attention\n",
    "<a id=part3_2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you've seen in class, the transformer model uses a Multi-head attention module. We will use the same implementation you've seen in the tutorial, aside from the attention mechanism itslef, which will be swapped with the sliding-window attention you implemented.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Insert the call to the sliding-window attention mechanism in the forward of MultiHeadAttention in hw3/transformer.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyHo9Oy_Z7SX",
    "tags": []
   },
   "source": [
    "## Sentiment analysis\n",
    "<a id=part3_3></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6MMtJFRaZCi"
   },
   "source": [
    "We will now go on to tackling the task of sentiment analysis which is the process of analyzing text to determine if the emotional tone of the message is positive or negative (many times a neutral class is also used, but this won't be the case in the data we will be working with).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### IMBD hugging face dataset\n",
    "<a id=part3_3_1></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging Face is a popular open-source library and platform that provides state-of-the-art tools and resources for natural language processing (NLP) tasks. It has gained immense popularity within the NLP community due to its user-friendly interfaces, powerful pre-trained models, and a vibrant community that actively contributes to its development. \n",
    "\n",
    "Hugging Face provides a wide array of tools and utilities, which we will leverage as well. The Hugging Face Transformers library, built on top of PyTorch and TensorFlow, offers a simple yet powerful API for working with Transformer-based models (such as Distil-BERT). It enables users to easily load, fine-tune, and evaluate models, as well as generate text using these models.\n",
    "\n",
    "Furthermore, Hugging Face offers the Hugging Face Datasets library, which provides access to a vast collection of publicly available datasets for NLP. These datasets can be conveniently downloaded and used for training and evaluation purposes.\n",
    "\n",
    "You are encouraged to visit their site and see other uses: https://huggingface.co/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import pathlib\n",
    "import urllib\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "from datasets import load_dataset, load_metric, concatenate_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the dataset using Hugging Face's `datasets` library.\n",
    "\n",
    "Feel free to look around at the full array of datasets that they offer.\n",
    "\n",
    "https://huggingface.co/docs/datasets/index\n",
    "\n",
    "We will load the full training and test sets in addition to a small toy subset of the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/ilay.kamai/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b31381fad66d437aa5ef36bdc5330b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('imdb', split=['train', 'test', 'train[12480:12520]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 25000\n",
      "}), Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 25000\n",
      "}), Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 40\n",
      "})]\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that it returned a list of 3 labeled datasets, the first two of size 25,000, and the third of size 40.\n",
    "We will use these as `train` and `test` datasets for training the model, and the `toy` dataset for a sanity check. \n",
    "These Datasets are wrapped in a `Dataset` class.\n",
    "\n",
    "We now wrap the dataset into a `DatasetDict` class, which contains helpful methods to use for working with the data.   \n",
    "https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#wrap it in a DatasetDict to enable methods such as map and format\n",
    "dataset = DatasetDict({'train': dataset[0], 'val': dataset[1], 'toy': dataset[2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    toy: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 40\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now access the datasets in the Dict as we would a dictionary.\n",
    "Let's print a few training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 25000\n",
      "})\n",
      "TRAINING SAMPLE 0:\n",
      "I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.\n",
      "Label 0: 0\n",
      "\n",
      "\n",
      "TRAINING SAMPLE 1:\n",
      "\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn't matter what one's political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn't true. I've seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don't exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we're treated to the site of Vincent Gallo's throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won't see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women's bodies.\n",
      "Label 1: 0\n",
      "\n",
      "\n",
      "TRAINING SAMPLE 2:\n",
      "If only to avoid making this type of film in the future. This film is interesting as an experiment but tells no cogent story.<br /><br />One might feel virtuous for sitting thru it because it touches on so many IMPORTANT issues but it does so without any discernable motive. The viewer comes away with no new perspectives (unless one comes up with one while one's mind wanders, as it will invariably do during this pointless film).<br /><br />One might better spend one's time staring out a window at a tree growing.<br /><br />\n",
      "Label 2: 0\n",
      "\n",
      "\n",
      "TRAINING SAMPLE 3:\n",
      "This film was probably inspired by Godard's Masculin, féminin and I urge you to see that film instead.<br /><br />The film has two strong elements and those are, (1) the realistic acting (2) the impressive, undeservedly good, photo. Apart from that, what strikes me most is the endless stream of silliness. Lena Nyman has to be most annoying actress in the world. She acts so stupid and with all the nudity in this film,...it's unattractive. Comparing to Godard's film, intellectuality has been replaced with stupidity. Without going too far on this subject, I would say that follows from the difference in ideals between the French and the Swedish society.<br /><br />A movie of its time, and place. 2/10.\n",
      "Label 3: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'])\n",
    "\n",
    "for i in range(4):\n",
    "    print(f'TRAINING SAMPLE {i}:') \n",
    "    print(dataset['train'][i]['text'])\n",
    "    label = dataset['train'][i]['label']\n",
    "    print(f'Label {i}: {label}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should check the label distirbution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative samples in train dataset: 12500\n",
      "positive samples in train dataset: 12500\n",
      "negative samples in val dataset: 12500\n",
      "positive samples in val dataset: 12500\n",
      "negative samples in toy dataset: 20\n",
      "positive samples in toy dataset: 20\n"
     ]
    }
   ],
   "source": [
    "def label_cnt(type):\n",
    "    ds = dataset[type]\n",
    "    size = len(ds)\n",
    "    cnt= 0 \n",
    "    for smp in ds:\n",
    "        cnt += smp['label']\n",
    "    print(f'negative samples in {type} dataset: {size - cnt}')\n",
    "    print(f'positive samples in {type} dataset: {cnt}')\n",
    "    \n",
    "label_cnt('train')\n",
    "label_cnt('val')\n",
    "label_cnt('toy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### __Import the tokenizer for the dataset__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s tokenize the texts into individual word tokens using the tokenizer implementation inherited from the pre-trained model class.  \n",
    "With Hugging Face you will always find a tokenizer associated with each model. If you are not doing research or experiments on tokenizers it’s always preferable to use the standard tokenizers.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4794099b1c084197948aa3da3ac3c199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/442 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "990ab2eaa0224209bb556ecb8be86ba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer input max length: 512\n",
      "Tokenizer vocabulary size: 30522\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "print(\"Tokenizer input max length:\", tokenizer.model_max_length)\n",
    "print(\"Tokenizer vocabulary size:\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create helper functions to tokenize the text. Notice the arguments sent to the tokenizer.  \n",
    "__Padding__ is a strategy for ensuring tensors are rectangular by adding a special padding token to shorter sentences.   \n",
    "On the other hand , sometimes a sequence may be too long for a model to handle. In this case, you’ll need to __truncate__ the sequence to a shorter length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_text(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=True)\n",
    "\n",
    "def tokenize_dataset(dataset):\n",
    "    dataset_tokenized = dataset.map(tokenize_text, batched=True, batch_size =None)\n",
    "    return dataset_tokenized\n",
    "\n",
    "dataset_tokenized = tokenize_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we would like to work with pytorch so we can manually fine-tune\n",
    "dataset_tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# no need to parrarelize in this assignment\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### __Setting up the dataloaders and dataset__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now set up the dataloaders for efficient batching and loading of the data.  \n",
    "By now, you are familiar with the Class methods that are needed to create a working Dataloader.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.ds = dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.ds[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.ds.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = IMDBDataset(dataset_tokenized['train'])\n",
    "val_dataset = IMDBDataset(dataset_tokenized['val'])\n",
    "toy_dataset = IMDBDataset(dataset_tokenized['toy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dl_train,dl_val, dl_toy = [ \n",
    "    DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=12,\n",
    "    shuffle=True, \n",
    "    num_workers=0\n",
    "),\n",
    "DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=12,\n",
    "    shuffle=True, \n",
    "    num_workers=0\n",
    "),\n",
    "DataLoader(\n",
    "    dataset=toy_dataset,\n",
    "    batch_size=4,\n",
    "    num_workers=0\n",
    ")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDSwVcumaY_E",
    "tags": []
   },
   "source": [
    "### Transformer Encoder\n",
    "<a id=part3_3_2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDsr_vqWa3V5"
   },
   "source": [
    "The model we will use for the task at hand, is the encoder of the transformer proposed in the seminal paper 'Attention Is All You Need'.\n",
    "\n",
    "The encoder is composed of positional encoding, and then multiple blocks which compute multi-head attention, layer normalization and a feed forward network as described in the diagram below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"imgs/transformer_encoder.png\" alt=\"Alternative text\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "We provided you with implemetations for the positional encoding and the position-wise feed forward MLP in hw3/transformer.py. \n",
    "\n",
    "Feel free to read through the implementations to make sure you understand what they do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: To begin with, complete the transformer EncoderLayer in hw3/transformer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 12, 16])\n",
      "long_diag: tensor([[[-2.9272e-09,  4.5775e-41,  1.3404e-02, -1.0781e-01, -8.6344e-02],\n",
      "         [-2.7665e-09,  4.5775e-41,  1.8327e-02, -6.8433e-02, -1.5318e-02],\n",
      "         [-2.7516e-09,  4.5775e-41, -2.2852e-02,  1.8116e-02,  2.5066e-02],\n",
      "         [-2.7605e-09,  4.5775e-41,  1.2843e-02,  4.7287e-02,  3.6280e-02]],\n",
      "\n",
      "        [[ 4.5775e-41, -3.6972e-02,  1.3981e-01, -3.7471e-02,  7.5612e-02],\n",
      "         [ 4.5775e-41,  2.2270e-02, -4.7699e-05,  1.3269e-01,  1.9791e-02],\n",
      "         [ 4.5775e-41, -2.8535e-02, -8.8936e-02, -5.9028e-02, -7.3630e-02],\n",
      "         [ 4.5775e-41, -4.8557e-03,  4.2000e-02,  4.2749e-02,  5.7861e-02]],\n",
      "\n",
      "        [[-3.9357e-02,  1.6660e-01,  3.0184e-02,  1.1102e-01, -3.7865e-02],\n",
      "         [ 4.6567e-02,  4.3682e-02,  1.8593e-01,  3.4032e-02,  1.2512e-01],\n",
      "         [-2.6391e-02, -5.0824e-02, -1.8818e-02, -4.4831e-02, -3.4237e-02],\n",
      "         [ 2.6523e-02,  1.1302e-01,  8.7175e-02,  1.4646e-01,  5.0429e-02]],\n",
      "\n",
      "        [[ 1.0697e-01, -3.1392e-02,  6.1033e-02, -6.9571e-02, -1.3632e-02],\n",
      "         [-5.0561e-02,  9.3741e-02, -1.7064e-02,  4.6233e-02, -5.7713e-02],\n",
      "         [-8.7154e-02, -6.1774e-02, -6.8015e-02, -5.8708e-02, -4.7424e-02],\n",
      "         [ 1.6039e-03, -1.4525e-02, -3.6502e-02, -1.3679e-02, -8.4080e-02]],\n",
      "\n",
      "        [[-2.9194e-02,  2.5718e-03, -5.0987e-02, -9.6651e-02, -4.1724e-02],\n",
      "         [ 1.0067e-01,  1.2870e-02,  8.9591e-02,  2.7834e-02, -6.7309e-02],\n",
      "         [-4.8450e-02, -7.1800e-02, -5.1680e-02, -7.6912e-02, -5.2362e-02],\n",
      "         [ 1.3705e-01,  2.1368e-01,  1.4118e-02,  2.8295e-02,  9.8083e-02]],\n",
      "\n",
      "        [[ 1.0863e-01, -2.8299e-02,  4.4587e-02,  7.9426e-02,  7.0263e-02],\n",
      "         [ 8.0189e-04,  2.3938e-02, -3.4395e-02, -1.2614e-01, -2.3430e-02],\n",
      "         [-4.1126e-02, -4.3009e-02, -3.9968e-02, -3.6978e-02, -1.1418e-01],\n",
      "         [ 2.1674e-01, -7.5852e-03, -4.4670e-02,  1.0042e-01,  9.9718e-02]],\n",
      "\n",
      "        [[-1.2563e-01, -1.6890e-01, -8.9108e-02, -1.1651e-01, -4.0718e-02],\n",
      "         [ 2.2746e-02, -7.4791e-02, -2.1168e-01, -1.4301e-01, -1.0790e-01],\n",
      "         [ 1.2980e-02,  2.7634e-02, -1.6194e-03, -1.2579e-02, -1.1489e-02],\n",
      "         [ 1.6203e-02, -6.4098e-04,  7.9996e-02,  5.7094e-02,  4.5297e-02]],\n",
      "\n",
      "        [[ 1.6115e-01,  2.0690e-01,  1.4842e-01,  3.1435e-01,  2.1248e-01],\n",
      "         [-4.7578e-02, -1.5490e-01, -1.4709e-01, -9.8359e-02,  4.3092e-02],\n",
      "         [-8.6653e-02, -7.6377e-02, -1.3694e-01, -1.1403e-01, -1.6726e-01],\n",
      "         [-7.0679e-02,  8.4175e-02,  3.7343e-02, -8.8981e-03,  6.6230e-02]],\n",
      "\n",
      "        [[ 1.0209e-01,  1.7054e-02,  2.1078e-01,  1.8720e-01,  8.3854e-02],\n",
      "         [-2.0638e-01, -9.0065e-02, -4.4848e-02,  8.4395e-02,  1.8693e-02],\n",
      "         [-3.9113e-02, -7.8426e-02, -6.2794e-02, -5.8044e-02, -5.2727e-02],\n",
      "         [ 8.9652e-02,  4.6664e-02,  3.4189e-02,  8.7106e-02,  5.1612e-02]],\n",
      "\n",
      "        [[-3.3153e-02,  6.0398e-02,  9.6431e-02,  1.5831e-02, -2.8382e-02],\n",
      "         [-6.4669e-02, -2.6779e-02,  8.1293e-02,  2.4780e-02,  1.7902e-02],\n",
      "         [-8.6065e-02, -8.8118e-02, -3.0936e-02, -4.2489e-02, -1.4552e-02],\n",
      "         [ 1.1175e-01,  6.4242e-02,  1.3780e-01,  9.9083e-02,  1.1481e-02]],\n",
      "\n",
      "        [[ 7.5434e-02,  1.2293e-01,  1.9760e-02, -3.9146e-02, -1.3390e-01],\n",
      "         [-3.4678e-03,  8.4812e-02,  3.4663e-02,  3.7998e-02, -6.4570e-02],\n",
      "         [-4.7163e-02, -3.2867e-03, -1.6818e-02, -1.7481e-02, -1.0715e-03],\n",
      "         [ 8.9232e-02,  1.0641e-01,  8.9417e-02,  1.0779e-02,  6.0383e-02]],\n",
      "\n",
      "        [[-5.7143e-02, -8.3821e-02,  1.3404e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [-4.1061e-02, -3.9699e-02,  1.8327e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [ 4.6744e-02,  2.3130e-02, -2.2852e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [ 6.7464e-02,  5.8384e-02,  1.2843e-02,  0.0000e+00,  0.0000e+00]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "torch.Size([2, 4, 10, 4])\n",
      "initial torch.Size([2, 4, 10, 4])\n",
      "shapes  2 10 4 4\n",
      "diag torch.Size([8, 5, 2, 5]) \n",
      " tensor([[[   -inf,    -inf,  0.0963,  0.0775,  0.1423],\n",
      "         [   -inf,  0.2609,  0.1622,  0.3036, -0.1083]],\n",
      "\n",
      "        [[ 0.3601,  0.4347,  0.5195,  0.0993, -0.0513],\n",
      "         [ 0.1381,  0.1433,  0.0717, -0.1666,  0.0020]],\n",
      "\n",
      "        [[ 0.2384,  0.2494,  0.1419,  0.0006,  0.1358],\n",
      "         [ 0.1223, -0.0388, -0.0563,  0.2340,  0.0519]],\n",
      "\n",
      "        [[-0.4006, -0.0029,  0.0878, -0.0358, -0.1810],\n",
      "         [-0.0872, -0.0777, -0.1872, -0.2895, -0.1968]],\n",
      "\n",
      "        [[ 0.3565,  0.2360,  0.1683,  0.1828,  0.1662],\n",
      "         [ 0.0154,  0.0328, -0.0204,  0.0000,  0.0000]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "torch.Size([2, 4, 10, 10])\n",
      "attn tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8056,\n",
      "        0.7955], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# ! pip show setuptools\n",
    "from hw3.longformer import LongformerSelfAttention, LongformerConfig\n",
    "from hw3.transformer import MultiHeadAttention\n",
    "cfg = LongformerConfig(attention_window=[2], attention_dilation=[1], hidden_size=16, num_attention_heads=4)\n",
    "longformer_atn = LongformerSelfAttention(cfg, 0)\n",
    "x = torch.load(os.path.join('test_tensors','encoder_layer_input.pt'))\n",
    "x_ = F.pad(x, (0,0,1,1,0,0))\n",
    "print(x_.shape)\n",
    "vals, atn = longformer_atn(x_, output_attentions=True)\n",
    "\n",
    "my_atn = MultiHeadAttention(input_dim=16, embed_dim=16, num_heads=4, window_size=4)\n",
    "vals2, atn2 = my_atn(x,padding_mask=None, return_attention=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 10, 4])\n",
      "initial torch.Size([2, 4, 10, 4])\n",
      "shapes  2 10 4 4\n",
      "torch.Size([2, 4, 10, 10])\n",
      "attn tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8248,\n",
      "        0.8894], grad_fn=<SumBackward1>)\n",
      "tensor([[-8.6314e-01, -1.3206e+00,  2.2582e+00,  6.1223e-01, -3.0466e-01,\n",
      "         -4.3579e-01,  7.5172e-01, -3.8886e-02, -4.6540e-01,  1.3883e+00,\n",
      "         -9.7882e-01,  2.7794e-01,  6.3340e-01, -1.8550e+00,  4.0227e-01,\n",
      "         -6.1712e-02],\n",
      "        [ 9.1672e-02,  2.1224e-03,  9.7766e-01, -6.1002e-02,  1.1186e+00,\n",
      "         -1.1086e+00, -4.2433e-01,  1.0308e+00, -1.4952e+00,  5.3197e-01,\n",
      "         -1.2844e+00,  1.5659e+00, -1.5304e-01, -1.6016e+00, -5.3670e-01,\n",
      "          1.3462e+00],\n",
      "        [-1.3050e-01, -8.4810e-01,  2.6123e+00, -7.8044e-01,  1.4717e-01,\n",
      "         -1.4329e+00,  4.6728e-01, -6.4218e-01,  8.1657e-02, -4.0033e-01,\n",
      "         -1.8548e-01,  7.2281e-01,  1.0735e+00, -1.6163e+00,  7.8375e-01,\n",
      "          1.4770e-01],\n",
      "        [-7.0883e-01,  5.2971e-02,  1.4390e+00,  5.1448e-01,  2.1840e+00,\n",
      "         -1.7216e-02, -3.6132e-01, -3.7459e-01, -1.1424e+00,  2.4962e-01,\n",
      "         -1.1709e+00, -3.6490e-01,  9.4666e-02, -1.9787e+00,  6.0834e-01,\n",
      "          9.7585e-01],\n",
      "        [-7.9606e-01,  4.6887e-01,  1.0152e+00,  9.0548e-01,  1.2367e+00,\n",
      "         -1.3803e+00, -1.0424e+00, -9.7073e-01, -1.1582e+00,  1.4337e+00,\n",
      "         -1.5495e-01, -6.2135e-01,  1.7385e+00, -9.5204e-01, -3.0677e-02,\n",
      "          3.0823e-01],\n",
      "        [-9.9987e-01, -1.6924e+00,  1.7438e+00,  7.5787e-01,  1.1466e+00,\n",
      "         -1.7700e+00, -6.7822e-01, -6.3797e-01, -1.3642e-02,  3.9247e-01,\n",
      "         -2.9670e-01, -1.5137e-01,  1.2266e+00, -4.8216e-01,  4.5792e-01,\n",
      "          9.9705e-01],\n",
      "        [-4.2249e-01, -1.0037e+00,  1.1074e-01,  8.6670e-01,  7.6385e-01,\n",
      "         -6.8268e-01, -1.1942e+00,  1.0162e-01, -1.6671e-01, -1.7263e-01,\n",
      "          8.9206e-02, -7.6729e-01,  1.1417e+00, -1.9346e+00,  1.1274e+00,\n",
      "          2.1431e+00],\n",
      "        [-3.5301e-01, -1.5794e+00,  7.3875e-01,  1.3359e-01,  3.8160e-02,\n",
      "         -1.0734e+00, -1.6119e+00,  8.6940e-01,  4.8041e-01,  2.1151e+00,\n",
      "         -1.1656e+00,  5.2727e-01, -5.8070e-01,  6.4125e-01, -2.5469e-01,\n",
      "          1.0748e+00],\n",
      "        [-3.1826e-01, -1.3786e+00,  1.1538e+00,  1.1585e+00,  3.2110e-02,\n",
      "         -1.3791e+00,  4.1769e-01,  1.5859e-01, -1.2361e+00, -1.1492e-01,\n",
      "         -1.2416e+00,  1.8612e+00,  8.9959e-02, -9.1546e-01,  1.2823e+00,\n",
      "          4.2993e-01],\n",
      "        [-1.5603e+00, -2.2011e+00,  4.2784e-01, -4.2859e-01,  1.3258e+00,\n",
      "         -4.2983e-01,  5.6672e-01, -3.8802e-01,  1.9816e-01,  1.6898e+00,\n",
      "         -9.5676e-01,  7.6293e-01,  5.5544e-01, -5.7601e-01,  6.3612e-02,\n",
      "          9.5024e-01]], grad_fn=<SelectBackward0>)\n",
      "tensor([[4.1020e-01, 3.0723e-01, 8.0060e-01, 9.5739e-01, 9.1955e-02, 7.7014e-01,\n",
      "         8.4477e-01, 5.8739e-01, 5.0589e-01, 8.8476e-01, 5.1482e-01, 3.7529e-01,\n",
      "         4.6308e-01, 6.6042e-01, 6.2517e-01, 4.7302e-01],\n",
      "        [6.5783e-01, 6.3466e-01, 9.4971e-02, 6.2459e-01, 8.0592e-01, 5.5734e-01,\n",
      "         4.5436e-01, 8.8302e-01, 6.3159e-02, 2.7178e-01, 2.7400e-01, 9.3203e-01,\n",
      "         1.6389e-02, 5.4959e-01, 1.5655e-02, 4.8356e-01],\n",
      "        [6.8637e-01, 3.8307e-01, 9.8537e-01, 5.0159e-01, 2.4508e-01, 6.2893e-01,\n",
      "         7.6003e-01, 4.9901e-01, 9.2774e-01, 1.8354e-01, 7.1334e-01, 7.8392e-01,\n",
      "         7.4835e-01, 4.9982e-01, 6.4273e-01, 1.0774e-01],\n",
      "        [1.0762e-01, 4.6892e-01, 4.3733e-01, 8.7126e-01, 8.6306e-01, 8.3902e-01,\n",
      "         2.7098e-01, 1.0748e-01, 3.1001e-02, 6.8096e-01, 4.9459e-02, 8.7076e-04,\n",
      "         2.4731e-01, 7.7848e-02, 3.0296e-01, 1.8523e-01],\n",
      "        [2.4348e-01, 9.7536e-01, 4.0578e-01, 9.1477e-01, 5.1109e-01, 4.3570e-01,\n",
      "         3.5361e-01, 1.4259e-01, 9.8276e-02, 8.4621e-01, 3.4592e-01, 1.2510e-02,\n",
      "         9.5886e-01, 5.0927e-01, 1.6542e-01, 1.0697e-01],\n",
      "        [8.0107e-02, 6.1572e-02, 9.5246e-01, 8.4475e-01, 5.4090e-01, 7.3863e-02,\n",
      "         3.6922e-01, 3.0303e-02, 6.7231e-01, 3.6724e-01, 3.9300e-01, 3.4399e-01,\n",
      "         7.3443e-01, 7.7064e-01, 6.0953e-01, 4.3739e-01],\n",
      "        [4.5059e-01, 4.9126e-01, 1.2767e-01, 9.8788e-01, 2.8436e-01, 7.9090e-01,\n",
      "         2.1970e-01, 5.0336e-01, 6.7116e-01, 4.0910e-02, 7.2935e-01, 1.1728e-01,\n",
      "         5.6977e-01, 4.0085e-01, 7.8661e-01, 9.8595e-01],\n",
      "        [6.1268e-01, 4.8572e-01, 2.5228e-01, 6.8642e-01, 2.5934e-01, 7.0653e-01,\n",
      "         2.8329e-01, 8.3916e-01, 8.8352e-01, 8.0659e-01, 5.0501e-01, 7.3187e-01,\n",
      "         4.4384e-02, 9.0549e-01, 3.7009e-01, 6.8228e-01],\n",
      "        [4.1533e-01, 3.3211e-01, 2.6520e-01, 8.2251e-01, 2.2798e-01, 4.1183e-01,\n",
      "         7.5223e-01, 5.5039e-01, 7.9300e-02, 2.8126e-03, 2.5691e-01, 9.6116e-01,\n",
      "         1.4449e-01, 7.7557e-01, 8.5791e-01, 2.9275e-01],\n",
      "        [2.7920e-01, 1.4028e-01, 5.7799e-01, 4.3010e-01, 5.2429e-01, 7.1688e-01,\n",
      "         7.0992e-01, 3.4225e-01, 5.3434e-01, 6.1877e-01, 5.1064e-01, 6.0006e-01,\n",
      "         2.9746e-01, 9.0984e-01, 4.4871e-01, 4.3603e-01]])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "False is not true : output of encoder layer is incorrect",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(out[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(x[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 18\u001b[0m \u001b[43mtest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massertTrue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput of encoder layer is incorrect\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/cs236781-hw/lib/python3.8/unittest/case.py:765\u001b[0m, in \u001b[0;36mTestCase.assertTrue\u001b[0;34m(self, expr, msg)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m expr:\n\u001b[1;32m    764\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_formatMessage(msg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not true\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m safe_repr(expr))\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfailureException(msg)\n",
      "\u001b[0;31mAssertionError\u001b[0m: False is not true : output of encoder layer is incorrect"
     ]
    }
   ],
   "source": [
    "from hw3.transformer import EncoderLayer\n",
    "# set torch seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "layer = EncoderLayer(embed_dim=16, hidden_dim=16, num_heads=4, window_size=4, dropout=0.1)\n",
    "\n",
    "# load x and y\n",
    "x = torch.load(os.path.join('test_tensors','encoder_layer_input.pt'))\n",
    "y = torch.load(os.path.join('test_tensors','encoder_layer_output.pt'))\n",
    "padding_mask = torch.ones(2, 10)\n",
    "padding_mask[:, 5:] = 0\n",
    "\n",
    "\n",
    "# forward pass\n",
    "out = layer(x, None)\n",
    "# out_reg = attention()\n",
    "print(out[0])\n",
    "print(x[0])\n",
    "test.assertTrue(torch.allclose(out, y, atol=1e-6), 'output of encoder layer is incorrect')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to classify a sentence using the encoder, we need to somehow summarize the output of the last encoder layer (which will include an output for each token in the tokenized input sentence). \n",
    "\n",
    "There are several options for doing this. We will use the output of the special token [CLS] appended to the beginning of each sentence by the bert tokenizer we are using.\n",
    "\n",
    "Let's see an example of the first tokens in a sentence after tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(dataset_tokenized['train'][0]['input_ids'])[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**TODO**: Now it's time to put it all together. Complete the implementaion of 'Encoder' in hw3/transformer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from hw3.transformer import Encoder\n",
    "\n",
    "# set torch seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "encoder = Encoder(vocab_size=100, embed_dim=16, num_heads=4, num_layers=3, \n",
    "                  hidden_dim=16, max_seq_length=64, window_size=4, dropout=0.1)\n",
    "\n",
    "\n",
    "# load x and y\n",
    "x = torch.load(os.path.join('test_tensors','encoder_input.pt'))\n",
    "y = torch.load(os.path.join('test_tensors','encoder_output.pt'))\n",
    "x = torch.randint(0, 100, (2, 64)).long()\n",
    "\n",
    "padding_mask = torch.ones(2, 64)\n",
    "padding_mask[:, 50:] = 0\n",
    "\n",
    "# forward pass\n",
    "out = encoder(x, padding_mask)\n",
    "test.assertTrue(torch.allclose(out, y, atol=1e-6), 'output of encoder layer is incorrect')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the encoder\n",
    "<a id=part3_3_3></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now proceed to train the model. \n",
    "\n",
    "**TODO**: Complete the implementation of TransformerEncoderTrainer in hw3/training.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training on a toy dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with, we will train on a small toy dataset of 40 samples. This will serve as a sanity check to make sure nothing is buggy.\n",
    "\n",
    "**TODO**: choose the hyperparameters in hw3.answers part3_transformer_encoder_hyperparams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VWHBI8gm2qo-"
   },
   "outputs": [],
   "source": [
    "from hw3.answers import part3_transformer_encoder_hyperparams\n",
    "\n",
    "params = part3_transformer_encoder_hyperparams()\n",
    "print(params)\n",
    "embed_dim = params['embed_dim'] \n",
    "num_heads = params['num_heads']\n",
    "num_layers = params['num_layers']\n",
    "hidden_dim = params['hidden_dim']\n",
    "window_size = params['window_size']\n",
    "dropout = params['droupout']\n",
    "lr = params['lr']\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "max_seq_length = tokenizer.model_max_length\n",
    "\n",
    "max_batches_per_epoch = None\n",
    "N_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_model = Encoder(vocab_size, embed_dim, num_heads, num_layers, hidden_dim, max_seq_length, window_size, dropout=dropout).to(device)\n",
    "toy_optimizer = optim.Adam(toy_model.parameters(), lr=lr)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fit your model\n",
    "import pickle\n",
    "if not os.path.exists('toy_transfomer_encoder.pt'):\n",
    "    # overfit\n",
    "    from hw3.training import TransformerEncoderTrainer\n",
    "    toy_trainer = TransformerEncoderTrainer(toy_model, criterion, toy_optimizer)\n",
    "    # set max batches per epoch\n",
    "    _ = toy_trainer.fit(dl_toy, dl_toy, N_EPOCHS, checkpoints='toy_transfomer_encoder', max_batches=max_batches_per_epoch)\n",
    "\n",
    "    \n",
    "\n",
    "toy_saved_state = torch.load('toy_transfomer_encoder.pt')\n",
    "toy_best_acc = toy_saved_state['best_acc']\n",
    "toy_model.load_state_dict(toy_saved_state['model_state']) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test.assertTrue(toy_best_acc >= 95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training on all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You are now ready to train your sentiment analysis classifier!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_batches_per_epoch = 500\n",
    "N_EPOCHS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Encoder(vocab_size, embed_dim, num_heads, num_layers, hidden_dim, max_seq_length, window_size, dropout).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fit your model\n",
    "import pickle\n",
    "if not os.path.exists('trained_transfomer_encoder.pt'):\n",
    "    from hw3.training import TransformerEncoderTrainer\n",
    "    trainer = TransformerEncoderTrainer(model, criterion, optimizer)\n",
    "    # set max batches per epoch\n",
    "    _ = trainer.fit(dl_train, dl_val, N_EPOCHS, checkpoints='trained_transfomer_encoder', max_batches=max_batches_per_epoch)\n",
    "    \n",
    "\n",
    "saved_state = torch.load('trained_transfomer_encoder.pt')\n",
    "best_acc = saved_state['best_acc']\n",
    "model.load_state_dict(saved_state['model_state']) \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test.assertTrue(best_acc >= 65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the follwing cells to see an example of the model output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rand_index = torch.randint(len(dataset_tokenized['val']), (1,))\n",
    "rand_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample = dataset['val'][rand_index]\n",
    "sample['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_sample = dataset_tokenized['val'][rand_index]\n",
    "tokenized_sample\n",
    "input_ids = tokenized_sample['input_ids'].to(device)\n",
    "label = tokenized_sample['label'].to(device)\n",
    "attention_mask = tokenized_sample['attention_mask'].to(float).to(device)\n",
    "\n",
    "print('label', label.shape)\n",
    "print('attention_mask', attention_mask.shape)\n",
    "prediction = model.predict(input_ids, attention_mask).squeeze(0)\n",
    "\n",
    "print('label: {}, prediction: {}'.format(label, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next part you wil see how to fine-tune a pretrained model for the same task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from cs236781.answers import display_answer\n",
    "import hw3.answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill your answers in hw3.answers.part3_q1 and hw3.answers.part3_q2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain why stacking encoder layers that use the sliding-window attention results in a broader context in the final layer.\n",
    "Hint: Think what happens when stacking CNN layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_answer(hw3.answers.part3_q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Propose a variation of the attention pattern such that the computational complexity stays similar to that of the sliding-window attention O(nw), but the attention is computed on a more global context.\n",
    "Note: There is no single correct answer to this, feel free to read the paper that proposed the sliding-window. Any solution that makes sense will be considered correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_answer(hw3.answers.part3_q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
